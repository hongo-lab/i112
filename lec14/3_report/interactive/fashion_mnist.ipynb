{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Training Only (PyTorch)\n",
    "- CPU/GPU自動切替\n",
    "- ハイパーパラメータ調整可\n",
    "- 学習時間の計測\n",
    "- 各エポック進捗表示\n",
    "- 学習後に評価と学習曲線のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# ハイパーパラメータ\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "learning_rate = 0.01\n",
    "hidden_units = 32\n",
    "\n",
    "# デバイスの自動選択\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 26.4M/26.4M [01:50<00:00, 240kB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 36.2kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 4.42M/4.42M [00:19<00:00, 229kB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 5.15k/5.15k [00:00<00:00, 14.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワーク定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [0/7500], Loss: 2.3255\n",
      "Epoch [1/3], Step [100/7500], Loss: 0.5833\n",
      "Epoch [1/3], Step [200/7500], Loss: 0.9284\n",
      "Epoch [1/3], Step [300/7500], Loss: 0.7395\n",
      "Epoch [1/3], Step [400/7500], Loss: 0.9350\n",
      "Epoch [1/3], Step [500/7500], Loss: 1.0863\n",
      "Epoch [1/3], Step [600/7500], Loss: 0.9639\n",
      "Epoch [1/3], Step [700/7500], Loss: 1.1259\n",
      "Epoch [1/3], Step [800/7500], Loss: 0.2093\n",
      "Epoch [1/3], Step [900/7500], Loss: 0.5502\n",
      "Epoch [1/3], Step [1000/7500], Loss: 1.6667\n",
      "Epoch [1/3], Step [1100/7500], Loss: 0.5688\n",
      "Epoch [1/3], Step [1200/7500], Loss: 0.5946\n",
      "Epoch [1/3], Step [1300/7500], Loss: 1.1346\n",
      "Epoch [1/3], Step [1400/7500], Loss: 0.9137\n",
      "Epoch [1/3], Step [1500/7500], Loss: 1.2153\n",
      "Epoch [1/3], Step [1600/7500], Loss: 0.3409\n",
      "Epoch [1/3], Step [1700/7500], Loss: 0.8249\n",
      "Epoch [1/3], Step [1800/7500], Loss: 0.4206\n",
      "Epoch [1/3], Step [1900/7500], Loss: 0.8260\n",
      "Epoch [1/3], Step [2000/7500], Loss: 0.6309\n",
      "Epoch [1/3], Step [2100/7500], Loss: 0.9325\n",
      "Epoch [1/3], Step [2200/7500], Loss: 1.3804\n",
      "Epoch [1/3], Step [2300/7500], Loss: 0.5059\n",
      "Epoch [1/3], Step [2400/7500], Loss: 3.1914\n",
      "Epoch [1/3], Step [2500/7500], Loss: 0.4869\n",
      "Epoch [1/3], Step [2600/7500], Loss: 0.5710\n",
      "Epoch [1/3], Step [2700/7500], Loss: 0.7901\n",
      "Epoch [1/3], Step [2800/7500], Loss: 0.1213\n",
      "Epoch [1/3], Step [2900/7500], Loss: 1.4115\n",
      "Epoch [1/3], Step [3000/7500], Loss: 0.7075\n",
      "Epoch [1/3], Step [3100/7500], Loss: 0.2049\n",
      "Epoch [1/3], Step [3200/7500], Loss: 0.6178\n",
      "Epoch [1/3], Step [3300/7500], Loss: 0.8954\n",
      "Epoch [1/3], Step [3400/7500], Loss: 0.6104\n",
      "Epoch [1/3], Step [3500/7500], Loss: 1.0296\n",
      "Epoch [1/3], Step [3600/7500], Loss: 0.4803\n",
      "Epoch [1/3], Step [3700/7500], Loss: 0.3298\n",
      "Epoch [1/3], Step [3800/7500], Loss: 0.4624\n",
      "Epoch [1/3], Step [3900/7500], Loss: 1.3486\n",
      "Epoch [1/3], Step [4000/7500], Loss: 1.5419\n",
      "Epoch [1/3], Step [4100/7500], Loss: 1.0919\n",
      "Epoch [1/3], Step [4200/7500], Loss: 0.6670\n",
      "Epoch [1/3], Step [4300/7500], Loss: 0.2801\n",
      "Epoch [1/3], Step [4400/7500], Loss: 0.4582\n",
      "Epoch [1/3], Step [4500/7500], Loss: 1.1268\n",
      "Epoch [1/3], Step [4600/7500], Loss: 0.2805\n",
      "Epoch [1/3], Step [4700/7500], Loss: 0.3200\n",
      "Epoch [1/3], Step [4800/7500], Loss: 0.5931\n",
      "Epoch [1/3], Step [4900/7500], Loss: 0.8147\n",
      "Epoch [1/3], Step [5000/7500], Loss: 0.8244\n",
      "Epoch [1/3], Step [5100/7500], Loss: 0.3692\n",
      "Epoch [1/3], Step [5200/7500], Loss: 0.1801\n",
      "Epoch [1/3], Step [5300/7500], Loss: 0.6471\n",
      "Epoch [1/3], Step [5400/7500], Loss: 0.3039\n",
      "Epoch [1/3], Step [5500/7500], Loss: 0.4411\n",
      "Epoch [1/3], Step [5600/7500], Loss: 1.5219\n",
      "Epoch [1/3], Step [5700/7500], Loss: 0.3475\n",
      "Epoch [1/3], Step [5800/7500], Loss: 0.9801\n",
      "Epoch [1/3], Step [5900/7500], Loss: 0.5592\n",
      "Epoch [1/3], Step [6000/7500], Loss: 1.0321\n",
      "Epoch [1/3], Step [6100/7500], Loss: 0.1451\n",
      "Epoch [1/3], Step [6200/7500], Loss: 0.7843\n",
      "Epoch [1/3], Step [6300/7500], Loss: 0.9459\n",
      "Epoch [1/3], Step [6400/7500], Loss: 0.2767\n",
      "Epoch [1/3], Step [6500/7500], Loss: 0.7783\n",
      "Epoch [1/3], Step [6600/7500], Loss: 0.8364\n",
      "Epoch [1/3], Step [6700/7500], Loss: 0.5866\n",
      "Epoch [1/3], Step [6800/7500], Loss: 1.3873\n",
      "Epoch [1/3], Step [6900/7500], Loss: 2.8177\n",
      "Epoch [1/3], Step [7000/7500], Loss: 1.2120\n",
      "Epoch [1/3], Step [7100/7500], Loss: 0.8270\n",
      "Epoch [1/3], Step [7200/7500], Loss: 0.6598\n",
      "Epoch [1/3], Step [7300/7500], Loss: 0.2696\n",
      "Epoch [1/3], Step [7400/7500], Loss: 0.2312\n",
      "Epoch [1] complete. Avg loss: 0.7129\n",
      "Epoch [2/3], Step [0/7500], Loss: 0.7021\n",
      "Epoch [2/3], Step [100/7500], Loss: 0.8060\n",
      "Epoch [2/3], Step [200/7500], Loss: 1.0670\n",
      "Epoch [2/3], Step [300/7500], Loss: 0.8511\n",
      "Epoch [2/3], Step [400/7500], Loss: 0.8111\n",
      "Epoch [2/3], Step [500/7500], Loss: 0.7320\n",
      "Epoch [2/3], Step [600/7500], Loss: 0.8732\n",
      "Epoch [2/3], Step [700/7500], Loss: 0.8787\n",
      "Epoch [2/3], Step [800/7500], Loss: 1.1308\n",
      "Epoch [2/3], Step [900/7500], Loss: 0.5794\n",
      "Epoch [2/3], Step [1000/7500], Loss: 0.8013\n",
      "Epoch [2/3], Step [1100/7500], Loss: 0.2151\n",
      "Epoch [2/3], Step [1200/7500], Loss: 0.1752\n",
      "Epoch [2/3], Step [1300/7500], Loss: 0.3799\n",
      "Epoch [2/3], Step [1400/7500], Loss: 0.8261\n",
      "Epoch [2/3], Step [1500/7500], Loss: 0.8496\n",
      "Epoch [2/3], Step [1600/7500], Loss: 1.2494\n",
      "Epoch [2/3], Step [1700/7500], Loss: 0.4304\n",
      "Epoch [2/3], Step [1800/7500], Loss: 0.4700\n",
      "Epoch [2/3], Step [1900/7500], Loss: 0.7891\n",
      "Epoch [2/3], Step [2000/7500], Loss: 0.6141\n",
      "Epoch [2/3], Step [2100/7500], Loss: 0.3380\n",
      "Epoch [2/3], Step [2200/7500], Loss: 0.4030\n",
      "Epoch [2/3], Step [2300/7500], Loss: 0.8954\n",
      "Epoch [2/3], Step [2400/7500], Loss: 0.5355\n",
      "Epoch [2/3], Step [2500/7500], Loss: 0.9574\n",
      "Epoch [2/3], Step [2600/7500], Loss: 1.0235\n",
      "Epoch [2/3], Step [2700/7500], Loss: 0.6810\n",
      "Epoch [2/3], Step [2800/7500], Loss: 0.4751\n",
      "Epoch [2/3], Step [2900/7500], Loss: 0.8070\n",
      "Epoch [2/3], Step [3000/7500], Loss: 1.0726\n",
      "Epoch [2/3], Step [3100/7500], Loss: 0.3708\n",
      "Epoch [2/3], Step [3200/7500], Loss: 0.2032\n",
      "Epoch [2/3], Step [3300/7500], Loss: 0.7197\n",
      "Epoch [2/3], Step [3400/7500], Loss: 0.6896\n",
      "Epoch [2/3], Step [3500/7500], Loss: 0.4609\n",
      "Epoch [2/3], Step [3600/7500], Loss: 0.5563\n",
      "Epoch [2/3], Step [3700/7500], Loss: 0.1447\n",
      "Epoch [2/3], Step [3800/7500], Loss: 0.5603\n",
      "Epoch [2/3], Step [3900/7500], Loss: 1.0989\n",
      "Epoch [2/3], Step [4000/7500], Loss: 0.2000\n",
      "Epoch [2/3], Step [4100/7500], Loss: 2.2389\n",
      "Epoch [2/3], Step [4200/7500], Loss: 0.4492\n",
      "Epoch [2/3], Step [4300/7500], Loss: 0.4332\n",
      "Epoch [2/3], Step [4400/7500], Loss: 0.5626\n",
      "Epoch [2/3], Step [4500/7500], Loss: 0.4209\n",
      "Epoch [2/3], Step [4600/7500], Loss: 1.0440\n",
      "Epoch [2/3], Step [4700/7500], Loss: 0.8479\n",
      "Epoch [2/3], Step [4800/7500], Loss: 0.6312\n",
      "Epoch [2/3], Step [4900/7500], Loss: 1.0523\n",
      "Epoch [2/3], Step [5000/7500], Loss: 0.3868\n",
      "Epoch [2/3], Step [5100/7500], Loss: 0.9264\n",
      "Epoch [2/3], Step [5200/7500], Loss: 0.1111\n",
      "Epoch [2/3], Step [5300/7500], Loss: 0.2953\n",
      "Epoch [2/3], Step [5400/7500], Loss: 0.2741\n",
      "Epoch [2/3], Step [5500/7500], Loss: 0.7061\n",
      "Epoch [2/3], Step [5600/7500], Loss: 0.8663\n",
      "Epoch [2/3], Step [5700/7500], Loss: 0.8636\n",
      "Epoch [2/3], Step [5800/7500], Loss: 0.3274\n",
      "Epoch [2/3], Step [5900/7500], Loss: 0.5047\n",
      "Epoch [2/3], Step [6000/7500], Loss: 1.2614\n",
      "Epoch [2/3], Step [6100/7500], Loss: 0.8432\n",
      "Epoch [2/3], Step [6200/7500], Loss: 0.6858\n",
      "Epoch [2/3], Step [6300/7500], Loss: 0.3376\n",
      "Epoch [2/3], Step [6400/7500], Loss: 0.4253\n",
      "Epoch [2/3], Step [6500/7500], Loss: 0.8166\n",
      "Epoch [2/3], Step [6600/7500], Loss: 3.0111\n",
      "Epoch [2/3], Step [6700/7500], Loss: 0.2172\n",
      "Epoch [2/3], Step [6800/7500], Loss: 1.1627\n",
      "Epoch [2/3], Step [6900/7500], Loss: 0.5744\n",
      "Epoch [2/3], Step [7000/7500], Loss: 0.4670\n",
      "Epoch [2/3], Step [7100/7500], Loss: 0.3245\n",
      "Epoch [2/3], Step [7200/7500], Loss: 0.8044\n",
      "Epoch [2/3], Step [7300/7500], Loss: 0.0722\n",
      "Epoch [2/3], Step [7400/7500], Loss: 0.2625\n",
      "Epoch [2] complete. Avg loss: 0.6431\n",
      "Epoch [3/3], Step [0/7500], Loss: 0.5833\n",
      "Epoch [3/3], Step [100/7500], Loss: 0.2570\n",
      "Epoch [3/3], Step [200/7500], Loss: 0.4603\n",
      "Epoch [3/3], Step [300/7500], Loss: 0.9363\n",
      "Epoch [3/3], Step [400/7500], Loss: 0.0280\n",
      "Epoch [3/3], Step [500/7500], Loss: 0.7381\n",
      "Epoch [3/3], Step [600/7500], Loss: 0.4895\n",
      "Epoch [3/3], Step [700/7500], Loss: 0.5356\n",
      "Epoch [3/3], Step [800/7500], Loss: 0.5317\n",
      "Epoch [3/3], Step [900/7500], Loss: 0.3254\n",
      "Epoch [3/3], Step [1000/7500], Loss: 0.9009\n",
      "Epoch [3/3], Step [1100/7500], Loss: 0.5126\n",
      "Epoch [3/3], Step [1200/7500], Loss: 0.6237\n",
      "Epoch [3/3], Step [1300/7500], Loss: 0.9648\n",
      "Epoch [3/3], Step [1400/7500], Loss: 0.1732\n",
      "Epoch [3/3], Step [1500/7500], Loss: 0.2754\n",
      "Epoch [3/3], Step [1600/7500], Loss: 0.6472\n",
      "Epoch [3/3], Step [1700/7500], Loss: 1.0266\n",
      "Epoch [3/3], Step [1800/7500], Loss: 0.3575\n",
      "Epoch [3/3], Step [1900/7500], Loss: 0.6236\n",
      "Epoch [3/3], Step [2000/7500], Loss: 1.1495\n",
      "Epoch [3/3], Step [2100/7500], Loss: 1.9238\n",
      "Epoch [3/3], Step [2200/7500], Loss: 1.0581\n",
      "Epoch [3/3], Step [2300/7500], Loss: 0.6411\n",
      "Epoch [3/3], Step [2400/7500], Loss: 1.6562\n",
      "Epoch [3/3], Step [2500/7500], Loss: 0.6137\n",
      "Epoch [3/3], Step [2600/7500], Loss: 0.4769\n",
      "Epoch [3/3], Step [2700/7500], Loss: 0.3067\n",
      "Epoch [3/3], Step [2800/7500], Loss: 1.2693\n",
      "Epoch [3/3], Step [2900/7500], Loss: 0.4231\n",
      "Epoch [3/3], Step [3000/7500], Loss: 0.4932\n",
      "Epoch [3/3], Step [3100/7500], Loss: 0.1904\n",
      "Epoch [3/3], Step [3200/7500], Loss: 0.2115\n",
      "Epoch [3/3], Step [3300/7500], Loss: 0.6734\n",
      "Epoch [3/3], Step [3400/7500], Loss: 0.5757\n",
      "Epoch [3/3], Step [3500/7500], Loss: 0.2691\n",
      "Epoch [3/3], Step [3600/7500], Loss: 0.1301\n",
      "Epoch [3/3], Step [3700/7500], Loss: 0.8390\n",
      "Epoch [3/3], Step [3800/7500], Loss: 0.2254\n",
      "Epoch [3/3], Step [3900/7500], Loss: 1.4351\n",
      "Epoch [3/3], Step [4000/7500], Loss: 0.2392\n",
      "Epoch [3/3], Step [4100/7500], Loss: 0.5604\n",
      "Epoch [3/3], Step [4200/7500], Loss: 0.6277\n",
      "Epoch [3/3], Step [4300/7500], Loss: 0.6190\n",
      "Epoch [3/3], Step [4400/7500], Loss: 0.9082\n",
      "Epoch [3/3], Step [4500/7500], Loss: 0.7391\n",
      "Epoch [3/3], Step [4600/7500], Loss: 0.8119\n",
      "Epoch [3/3], Step [4700/7500], Loss: 0.3889\n",
      "Epoch [3/3], Step [4800/7500], Loss: 0.7498\n",
      "Epoch [3/3], Step [4900/7500], Loss: 0.1986\n",
      "Epoch [3/3], Step [5000/7500], Loss: 0.6051\n",
      "Epoch [3/3], Step [5100/7500], Loss: 0.4977\n",
      "Epoch [3/3], Step [5200/7500], Loss: 1.4679\n",
      "Epoch [3/3], Step [5300/7500], Loss: 0.8510\n",
      "Epoch [3/3], Step [5400/7500], Loss: 0.3234\n",
      "Epoch [3/3], Step [5500/7500], Loss: 0.9408\n",
      "Epoch [3/3], Step [5600/7500], Loss: 0.3274\n",
      "Epoch [3/3], Step [5700/7500], Loss: 0.7620\n",
      "Epoch [3/3], Step [5800/7500], Loss: 0.2554\n",
      "Epoch [3/3], Step [5900/7500], Loss: 0.2539\n",
      "Epoch [3/3], Step [6000/7500], Loss: 0.9877\n",
      "Epoch [3/3], Step [6100/7500], Loss: 0.4430\n",
      "Epoch [3/3], Step [6200/7500], Loss: 0.6178\n",
      "Epoch [3/3], Step [6300/7500], Loss: 0.3653\n",
      "Epoch [3/3], Step [6400/7500], Loss: 0.5100\n",
      "Epoch [3/3], Step [6500/7500], Loss: 0.3448\n",
      "Epoch [3/3], Step [6600/7500], Loss: 0.6363\n",
      "Epoch [3/3], Step [6700/7500], Loss: 0.6849\n",
      "Epoch [3/3], Step [6800/7500], Loss: 1.2966\n",
      "Epoch [3/3], Step [6900/7500], Loss: 0.8144\n",
      "Epoch [3/3], Step [7000/7500], Loss: 0.0736\n",
      "Epoch [3/3], Step [7100/7500], Loss: 0.4403\n",
      "Epoch [3/3], Step [7200/7500], Loss: 0.6085\n",
      "Epoch [3/3], Step [7300/7500], Loss: 0.8588\n",
      "Epoch [3/3], Step [7400/7500], Loss: 0.4158\n",
      "Epoch [3] complete. Avg loss: 0.6038\n",
      "\n",
      "Training completed in 19.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# 学習ループ\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}] complete. Avg loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining completed in {(end_time - start_time):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.39%\n"
     ]
    }
   ],
   "source": [
    "# 学習後にテスト精度を表示（オプション）\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
